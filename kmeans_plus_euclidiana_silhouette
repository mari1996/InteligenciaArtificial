# -*- coding: utf-8 -*-
"""
Created on Tue May  1 20:18:18 2018

@author: Mariana
"""
#kmeans++ com distancia euclidiana e usando silhouette

import pandas as pd
import numpy as np
import time
import datetime
import math
import random

#K-means implementado com distancia euclidiana
#funcao que calcula a distancia euclidiana entre dois vetores (de qualquer dimensao)
def distanciaEuclidiana (vetor1, vetor2):  
    
    tam = 0
    
    quar_distance = 0
    
    # Se vetor for multidimensional
    if(vetor1.ndim > 1):
        
        # linhas do vetor
        lin = vetor1.shape[0]
        
        # colunas do vetor
        col = vetor1.shape[1]
        
        # Enquanto houver cluster
        while tam < lin:
            
            index = 0
            
            # Enquanto houver dado
            while index < col:
                
                    # Soma a distancia entre os dados ao quadrado
                    quar_distance += (vetor1[tam][index] - vetor2[tam][index]) ** 2
                    
                    index = index + 1
                    
            tam = tam+1
            
    # Se o vetor for unidimensional
    else:
        
        # Quantidade de dados do vetor
        col = len(vetor1)
        
        index = 0
        
        # Enquanto houver dados
        while index < col:
            
            # Soma a distancia entre os dados ao quadrado
            quar_distance += (vetor1[index] - vetor2[index]) ** 2
            
            index = index + 1
            
        tam = tam+1
        
    # Retorna a raiz quadrada da soma, que e a distancia euclidiana    
    return math.sqrt(quar_distance)


#Como funciona o silhouette:
#Primeiro precisamos calcular a distancia media de cada dado i para todos os dados que estao no mesmo cluster
#esse vetor vai guardar a distância media de i para todos os dados que estao no mesmo cluster
#essa distancia sera calculada mais pra frente para todos os dados do conjunto de dados (no caso, de cada documento)
#o que acontece aqui e que veremos se cada documento esta bem ajustado em cada cluster
#implementacao do Silhouette para kmeans com distancia euclidiana
#para implementarmos o silhoutte
def silhouette (vetorNumeros, dados_cluster, numCluster):   
    
    #calculamos o total de dados que vamos ter que iterar
    totalDocs = vetorNumeros.shape[0]
    
    #criamos um vetor pra guardar quantos dados temos em cada cluster
    totalCluster = [0]*numCluster
    
    #iterador
    i = 0
    
    #criamos o vetor silhouette que vai guardar o valor de S pra cada dado
    #s(i) = (b(i) - a(i))))/(max{a(i), b(i})
    silhouette = [0]*totalDocs
    
    #vamos fazer a contagem de quantos elementos estao em cada cluster
    while i < totalDocs:
        
        #para cada documento que temos vamos verificar a qual cluster ele pertence
        #quando encontramos o cluster correto, iteramos em +1 a quantidade de documentos que pertencem a aquele cluster
        totalCluster[int(dados_cluster[i])] += 1
        
        i += 1
    
    dado = 0
    
    #criamos o vetor A que vai guardar o valor final da distancia media de cada dado para todos os dados que estao no mesmo 
    #cluster que ele
    A = [0]*totalDocs   
    
    #criamos um vetor com numDocs como numero de linhas porque vamos calcular a distancia media dos dados do mesmo cluster para 
    #cada um
    tempA = [0]*totalDocs
    
    #criamos o vetorB que vai guardar a distancia minima media entre o dado e todos os dados de cada cluster a que ele nao pertence
    B = [0]*totalDocs
    
    #criamos tambem um vetor temporario para ajudar nessas operacoes
    tempB = np.zeros((totalDocs, numCluster))
    
    #nesse laco vamos fazer a construcao do vetor S (silhouette)
    #para isso vamos passar por cada dado para calcular duas distancias
    #a primeira distancia e o calculo da distancia media do dado i a todos os outros dados que estao no mesmo cluster que ele
    #a segunda distancia e o calcula da minima distancia media do dado i a todos os outros dados que estao nos clusters em que 
    #ele nao esta
    while dado < totalDocs:
        
        #Calculos das distancias
        
        #iterador
        j = 0
        
        #primeiro: preciso saber se os dados pertencem ao mesmo cluster:
        #se sim, somo a distancia desses dados referente ao dado que eu estou trabalhando
        #se não, preciso calcular a distancia do dado i pra todos os dados de todos os outros clusters
        #e depois calcular a menor distancia media
        while j < totalDocs:
            
            if(dado != j and (int(dados_cluster[dado]) == int(dados_cluster[j]))):
                
                #significa que temos que calcular as distancias de todos os dados que estao no mesmo cluster     
                
                tempA[dado] += distanciaEuclidiana(vetorNumeros[dado], vetorNumeros[j])
                
            if(dado != j and (int(dados_cluster[dado]) != int(dados_cluster[j]))):
                
                #siginifica que temos que calcular a distancia do dados pra todos os outros dados dos outros clusters
                #e eleger a menor distancia media como "campea"
                
                tempB[dado][int(dados_cluster[j])] += distanciaEuclidiana(vetorNumeros[dado], vetorNumeros[j])
                
            j += 1
        
        #se o valor final de A para um determinado dado for igual a 0, nem fazemos conta
        if(tempA[dado] != 0):
    
            A[dado] = tempA[dado]/totalCluster[int(dados_cluster[dado])]
        
        k = dado
        l = 0
        
        #pra cada dado, vamos iterar para arrumar o vetor B
        #(iteramos de acordo com o total de clusters, que é a mesma quantidade de colunas que temos em tempB)
        while l < numCluster:
            
            #se tempB do dado i no cluster c for 0, significa que eles pertencem ao mesmo cluster, entao essas contas ja foram
            #feitas em A
            #nesse caso, para nao atrapalhar depois em que vamos tirar o minimo das distancias medias, colocamos um valor bem
            #alto para esse valor nunca ser pego e nao atrapalhar as contas
            if(tempB[k][l] == 0):
                
                tempB[k][l] = 9999
            
            else:
                
                #se existe valor, entao atualizamos o valor para a distancia media entre o dado e os dados de cada cluster
                tempB[k][l] = tempB[k][l]/totalCluster[l]
                
            l += 1
        
        #pegamos o minimo da distancia media
        B[dado] = np.min(tempB[dado])
        
        #Construcao do vetor Silhouette
        #fazer atualizacao do silhouette
        silhouette[dado] = (B[dado]-A[dado])/(max(A[dado],B[dado]))         
        
        dado += 1
    
    return silhouette
#funcao criada para pegar o conteudo que esta dentro do excel passado
def pegaTexto (nome):   
    return pd.read_excel(nome, encoding = 'iso-8859-1')

#a funcao inicializaCentroides e usada para inicializar os centroides no kmeans++
#funcionamento:
#de inicio estabelecemos um centroide inicial de forma aleatoria
#apos isso, vamos calcular a distancia entre todos os dados do conjunto para o centroide inicial
#vamos armazenar essas distancias em um vetor especifico
#depois disso, calculamos a soma de todos os elementos que estao nesse vetor de distancias
#feito isso, criamos um outro vetor que vai guardar a razao entre cada elemento do vetor de distancias pela soma que foi feita anteriormente
#depois disso, precisamos criar uma variavel aleatoria entre 0 e 1 que vai representar a probabilidade
#para esse algoritmo usamos o roulette wheel selection (ou fitness proportionate selection):
#no roulette wheel, vamos verificar se a soma de uma "parte" de um array (ou a soma de varias partes), ultrapassa um determinado
#valor (nesse caso, o valor da probabilidade)
#dessa forma, conseguimos determinar cada novo centroide de forma proporcional a sua probabilidade
#como parametros precisamos receber o array contendo todos os dados de entrada (os documentos e as palavras que representam eles),
#precisamos do numero de linhas e de colunas (quantidade de documentos e de palavras) e tambem do numero de clusters (pra saber quantos centroides vamos ter)
def inicializaCentroides(vetor_dados, num_linhas, num_clusters, num_colunas):
  
    #inicialmente criamos uma vetor que vai guardar todos os centroides (ele possui o numero de clusters como linhas e o total de colunas como o numero de palavras)
    centroides = np.zeros((num_clusters, num_colunas))

    #o primeiro centroide e inicializado aleatoriamente (escolhemos um valor que esta dentro do conjunto de dados)
    centroides[0] = vetor_dados[np.random.randint(0, num_linhas - 1, size=1)]
    
    #para determinar cada novo centroide (alem do que esta na posicao 0), precisamos iterar pelo vetor de centroides que possuimos
    for indice_cluster in range(0, num_clusters - 1):
    
        #no kmeans++, apos determinar um centroide, precisamos calcular a distancia entre todos os dados do conjunto 
        #e o centroide atual para conseguirmos determinar a posicao do proximo centroide
        #para isso, vamos iterar sobre todos os dados e calcular a distancia entre cada dado e o centroide em questao
        for indice_dado, dado in enumerate(vetor_dados):
            
            #criamos um vetor inicialmente com 0s para guardar a distancia entre cada dado do conjunto e o centroide atual
            #esse vetor e zerado a cada nova iteracao (ou a cada novo centroide)
            distanciasNormal = np.zeros((num_linhas, 1))
            
            #esse vetor guarda o valor da distancia entre cada dado e o centroide
            distanciasNormal[indice_dado] = distanciaEuclidiana(centroides[indice_cluster], dado)
        
        #no kmeans++, apos calcular a distancia entre cada dado e cada centroide, precisamos calcular a soma de todas essas
        #distancias para podermos calcular a posicao do novo centroide
        somaDistancias = np.sum(distanciasNormal)
        
        #criamos um vetor que vai guardar a razao entre a distancia entre cada dado para cada centroide/soma das distancias dos dados para aquela centroide
        #dessa forma vamos obter um vetor com distancias pequenas e que podem ser usadas para encontrar o proximo centroide
        distanciasProb = np.zeros((num_linhas, 1))
        
        #pra cada dado vamos encontrar a razao mencionada acima
        for indice_dado, dado in enumerate(vetor_dados):
            
            distanciasProb[indice_dado] = ((distanciasNormal[indice_dado])/(somaDistancias))

        #criamos uma variavel aleatoria que vai representar a probabilidade (os valores estao limitados entre 0 e 1 por conta da aplicação do roulette wheel)
        probabilidade = random.uniform(0, 1)
        
        #a principal vantagem do kmeans++ esta definida abaixo
        #verificamos, pra cada distancia (com razao) calculada acima, se aquele dado e um bom representante para o novo centroide
        #decidimos isso verificando o pedaco que o dado representa no vetor (a porcao que ele esta ocupando) e maior que a probabilidade definida
        #caso seja maior, significa que a porcao que ele ocupa no vetor de distancias e bem consideravel, entao a probabilidade de que esse
        #dado esteja distante do centroide e bem alta - dessa forma esse novo dado sera o proximo centroide da lista (se estamos iterando no 
        #centroide i, o centroide i + 1 sera esse dado)
        #caso esse dado nao tenha uma distancia que ultrapassa essa probabilidade (armazenada na variavel somaProbabilidade), apenas somamos a distancia a variavel somaProbabilidade
        somaProbabilidade = 0
        
        for indice_vetor, valor in enumerate(distanciasProb):
            
            if(somaProbabilidade > probabilidade):
                
                centroides[indice_cluster + 1] = vetor_dados[indice_vetor]
            
            else: 
                
                somaProbabilidade += distanciasProb[indice_vetor]
                    
    return centroides    

#funcao kmeans
def kmeans(num_clusters, taxa_erro, num_linhas, num_colunas, vetor_dados, max_iteracoes):           
    #criamos uma lista que vai guardar os centroides 
    #esses centroides vao ser inicializados de forma aleatoria
    
    centroides = inicializaCentroides(vetor_dados, num_linhas, num_clusters, num_colunas)

    #para podermos calcular o erro medio precisamos comparar a lista de centroides atuais com a lista anterior de centroides
    #essa lista é definida com o mesmo tamanho que a nossa lista de centroides
    #inicializamos esse vetor com 0s
    centroides_antigos = np.zeros(centroides.shape)
    
    #como cada dado vai pertencer a um cluster diferente, criamos uma lista que vai conter todos os dados e em quais
    #clusters eles vao estar
    #inicializamos esse vetor com 0s
    #vetor de dados x cluster
    dados_cluster = np.zeros((num_linhas, 1))
    
    #precisamos calcular a distancia entre os dois vetores de centroides, o antigo e o novo, para saber se diminuimos o erro ou nao
    #calculamos essa distancia usando a distancia euclidiana
    #se a distancia (ou erro) for menor que a taxa de erro passada, significa que nao precisamos mais calcular novos centroides
    distancia_centroides = distanciaEuclidiana(centroides, centroides_antigos)
    
    #guardamos o número de iterações pra não passarmos do máximo de iteracoes permitidas
    iteracoes = 0
    
    #vamos recalcular os centroides apenas se o erro ainda for maior do que a taxa permitida e se o numero maximo de iteracoes ainda 
    #nao foi alcancado
    #verificar se é or ou and aqui
    while iteracoes < max_iteracoes and distancia_centroides > taxa_erro:
        
        #a iteracao aumenta em 1
        iteracoes += 1
        
        #recalculamos a distancia
        distancia_centroides = distanciaEuclidiana(centroides, centroides_antigos)
        
        #como estamos numa nova iteracao, consideramos que os centroides atuais agora vao ser antigos
        #porque vamos recalcular a lista de centroides
        centroides_antigos = centroides
        
        #como vamos calcular os novos centroides precisamos saber a distancia de cada dado para cada centroide atual
        #para isso, criamos um vetor de distancias que vai guardar essas informacoes
        #a funcao enumerate enumera os dados que estamos vendo, nesse caso, os dados de vetor_dados
        #nesse caso, cada dado tera um indice proprio
        for indice_dado, dado in enumerate(vetor_dados):
            
            #definimos um vetor de tamanho num_clusters, pois temos num_clusters centroides que devem ser recalculados
            distancia_dados = np.zeros((num_clusters, 1))
            
            #para cada centroide que possuimos, precisamos calcular as distancias dos dados ate eles
            for indice_centroide, centroide in enumerate(centroides):
                
                #vamos calcular a distancia de cada dado ate cada centroide
                #as menores distancias vao mostrar onde os dados devem ficar
                distancia_dados[indice_centroide] = distanciaEuclidiana(centroide, dado)
                
            #essas distancias estao armazenadas num vetor distancia de dados X centroide
            #queremos identificar em qual cluster cada dado vai ficar
            #para isso, criamos uma lista contendo todos os dados e a qual cluster ele pertence (nessa iteracao)
            #dizemos que o dado X esta no cluster em que a distancia entre ele e o centroide N e a menor
            #a funcao np.argmin retorna o indice do menor valor
            dados_cluster[indice_dado, 0] = np.argmin(distancia_dados)
            
        #criamos uma lista de centroides temporarios que sao atualizados dentro dessa iteracao para nao perdermos o que 
        #estamos calculando
        temp_centroides = np.zeros((num_clusters, num_colunas))
        
        #para cada cluster vamos encontrar todos os dados que estao mais proximos dele e vamos achar o ponto medio entre eles
        #esse ponto medio sera o valor do novo centroide
        for indice in range(len(centroides)):
            
            dados_proximos = [i for i in range(len(dados_cluster)) if dados_cluster[i] == indice]   
            
            #se o valor da distancia for zero, definir esse centroide novo como o centroide antigo
            
            #tratamos o caso do cluster ainda não ter dados próximos a ele
            #se o cluster ainda não possui dados proximos a ele, pulamos esse caso
            if dados_proximos == []:
                
                break
            
            #encontramos o valor medio desses dados proximos, que vai ser o nosso novo centroide
            #adicionar um if pra não entrar no caso de ser 0
            centroide = np.mean(vetor_dados[dados_proximos], axis=0)
            
            #acrescentamos o novo centroide na lista de centroides temporarios
            temp_centroides[indice, :] = centroide
            
        centroides = temp_centroides
           
    return centroides, dados_cluster, iteracoes

##################################################################################################################################

arquivo = 'C:/Users/Mariana/Desktop/EP IA - Sarajane/Saidas_teste/Process_Mining_Abstracts_TF.xlsx'
#arquivo = 'C:/Users/Mariana/Desktop/IA/Saidas_teste/Process_Mining_Abstracts_Binario.xlsx' 
#arquivo = 'C:/Users/Mariana/Desktop/IA/Saidas_teste/TFIDF.xlsx'

#a matriz dados vai guardar todo o conteudo lido do arquivo, inclusive os cabecalhos (documentos e palavras)
dados = pegaTexto(arquivo)

#o vetor de palavras vai guardar o cabecalho "linha" que foi lido do Excel
palavras = dados.axes[1].tolist()

#o vetor de documentos vai guardar o cabecalho "coluna" que foi lido do Excel
documentos = dados.axes[0].tolist()

#a variavel "colunas" guarda a quantidade de colunas que existem no excel passado
colunas = dados.shape[1]

#a variavel "linhas" guarda a quantidade de linhas que existem no arquivo passado
linhas = dados.shape[0]

#o vetor conteudo guarda os valores numericos pegos no excel, ou seja, as aparicoes das palavras nos textos
conteudo = dados.values.tolist()

#esse vetor "valores" vai guardar todas as informacoes numericas que sao pegas no excel
#ou seja, esse vetor vai conter as aparicoes das palavras nos textos
valores = np.zeros((linhas, colunas))

a = 0

b = 0

#esse loop vai guardar os valores de conteudo em um outro vetor
while a < linhas:
    
    while b< colunas:
        
        valores[a][b] = conteudo[a][b]
        
        b+=1
        
    b = 1
    
    a+=1


#funcao que faz a escrita do log execucao
def escrita(arquivo_entrada, num_clusters, tempo_total, erro, docs, vetor_cluster, numArquivo, iteracoes):
    
    #essa funcao faz a escrita do log de execucao de cada chamada ao k-means e salva no seguinte formato:
    arquivo = open('C:/Users/Mariana/Desktop/EP IA - Sarajane/saidas/execucaoKMeansPlus'+str(numArquivo)+'.txt', 'w')
    
    now = datetime.datetime.now()
    
    arquivo.write('Execução feita em '+str(now.day)+'/'+str(now.month)+'/'+str(now.year)+' - '+str(now.hour)+':'+str(now.minute)+'\n')
    
    arquivo.write('\n')
    
    arquivo.write('Arquivo de entrada '+arquivo_entrada+'\n')
    
    arquivo.write('Numero de clusters: '+str(num_clusters)+'\n')
    
    arquivo.write('Tempo total de execução: '+str(tempo_total)+'\n')
    
    arquivo.write('Taxa de erro máxima permitida: '+str(erro)+'\n')
    
    arquivo.write('Total de iteracoes: '+str(iteracoes)+'\n')
    
    arquivo.write('Distância usada: euclidiana')
    
    arquivo.write('\n')
    
    arquivo.write('\n')
    
    #mandar qual documento ficou em cada cluster
    m = 0
    
    while m < len(docs):
        
        arquivo.write('Documento '+str(docs[m])+' - Cluster '+str(int(vetor_cluster.item(m)))+'\n')
        
        m += 1
     
    arquivo.close()
    
    return

##################################################################################################################################

#o execute fica aqui
#ele serve como um main que vai chamar varias vezes o kmeans, sempre variando os parametros usados    
def execute(clusters, erro, maxIter):

    
    start = time.time()

    i = 0
    
    while i < 5:
        

        #centroides, dados_cluster, iteracoes = kmeans(clusters, erro, linhas, colunas, valores, maxIter)
        centroides, dados_cluster, iteracoes = kmeans(clusters, erro, linhas, colunas, valores, maxIter)
       
        tempo_total = ("--- %s seconds ---" % (time.time() - start))
        escrita(arquivo, clusters,tempo_total,  erro, documentos, dados_cluster, i, iteracoes)
        
        i += 1
    teste = silhouette(valores, dados_cluster, clusters)

    return


num_clusters = 7

taxa_erro = 0.0

max_iteracoes = 50

execute(num_clusters, taxa_erro, max_iteracoes)
